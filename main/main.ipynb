{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from shap_e.diffusion.sample import sample_latents\n",
    "from shap_e.diffusion.gaussian_diffusion import diffusion_from_config\n",
    "from shap_e.models.download import load_model, load_config\n",
    "from shap_e.util.notebooks import create_pan_cameras, decode_latent_images, gif_widget, decode_latent_mesh\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "assert(device != 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAVE_LOCATION = 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xm = load_model('transmitter', device=device)\n",
    "model = load_model('text300M', device=device)\n",
    "diffusion = diffusion_from_config(load_config('diffusion'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generateLatentFromPrompt(prompt, guidance_scale=15.0):\n",
    "\n",
    "    latents = sample_latents(\n",
    "        batch_size=1,\n",
    "        model=model,\n",
    "        diffusion=diffusion,\n",
    "        guidance_scale=guidance_scale,\n",
    "        model_kwargs=dict(texts=[prompt] * 1),\n",
    "        progress=True,\n",
    "        clip_denoised=True,\n",
    "        use_fp16=True,\n",
    "        use_karras=True,\n",
    "        karras_steps=64,\n",
    "        sigma_min=1e-3,\n",
    "        sigma_max=160,\n",
    "        s_churn=0,\n",
    "    )\n",
    "    \n",
    "    return latents[0]\n",
    "\n",
    "def exportLatentToObj(latent, name):\n",
    "    t = decode_latent_mesh(xm, latent).tri_mesh()\n",
    "    with open(f'{SAVE_LOCATION}{name}.obj', 'w') as f:\n",
    "        t.write_obj(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0bf574476184baf8da6fd06bd19f811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a887a1f955453a9b492adf2bf7d3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompts = ['a shark', 'a goldfish']\n",
    "latents = []\n",
    "for prompt in prompts:\n",
    "    latents.append(generateLatentFromPrompt(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\almog\\dev\\shap-e\\shap_e\\models\\stf\\renderer.py:286: UserWarning: exception rendering with PyTorch3D: No module named 'pytorch3d'\n",
      "  warnings.warn(f\"exception rendering with PyTorch3D: {exc}\")\n",
      "c:\\users\\almog\\dev\\shap-e\\shap_e\\models\\stf\\renderer.py:287: UserWarning: falling back on native PyTorch renderer, which does not support full gradients\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "linear_interpolation_increments = 10\n",
    "\n",
    "z_interpolations = []\n",
    "for i in range(linear_interpolation_increments):\n",
    "    alpha = i * (1 / linear_interpolation_increments);\n",
    "    z_interpolations.append((1 - alpha) * latents[0] + alpha * latents[1])\n",
    "\n",
    "for i, z in enumerate(z_interpolations):\n",
    "    exportLatentToObj(z, f'{i}_shark_goldfish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prompt: a red car...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29768f80a42540bbb71c9ca63cb42f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f6429904ee4b749c66d8267ced66eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a9de6354c14106bad3b7a22d547c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78284c7ce34740b7b33c230282e9f16c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e818c530bf4be6847701c82fc980be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prompt: a blue car...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44404a8cee014fed9bce348c0a8cac75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d1c8a1c6254f33a22604c57a18d84b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0639230ea9344fda6d62369217c0654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03d5e9c72b2401ab6aac7e7d534bdbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1567f7582e2c4c3da17d9ae1524509de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_prompt = 'a ``` car'\n",
    "categories = ['red', 'blue']\n",
    "latents_per_category = 5\n",
    "\n",
    "samples = {category: [] for category in categories}\n",
    "for category in categories:\n",
    "    prompt = base_prompt.replace('```', category)\n",
    "    print(f'Starting prompt: {prompt}...')\n",
    "    for i in range(latents_per_category):\n",
    "        samples[category].append(generateLatentFromPrompt(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m latent_vectors, labels\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(color_epochs):\n\u001b[1;32m---> 32\u001b[0m     latent_vectors, labels \u001b[38;5;241m=\u001b[39m \u001b[43mrandomSample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolor_batch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     optimizerClassiferModel\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     34\u001b[0m     output \u001b[38;5;241m=\u001b[39m classiferModel(latent_vectors)\n",
      "Cell \u001b[1;32mIn[8], line 22\u001b[0m, in \u001b[0;36mrandomSample\u001b[1;34m(batch_size)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandomSample\u001b[39m(batch_size):\n\u001b[1;32m---> 22\u001b[0m     latent_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mred\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m random\u001b[38;5;241m.\u001b[39msample(samples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size)\n\u001b[0;32m     23\u001b[0m     latent_vectors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(latent_vectors)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     24\u001b[0m     labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size)] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size)], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\shape-e\\lib\\random.py:482\u001b[0m, in \u001b[0;36mRandom.sample\u001b[1;34m(self, population, k, counts)\u001b[0m\n\u001b[0;32m    480\u001b[0m randbelow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_randbelow\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m k \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n:\n\u001b[1;32m--> 482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample larger than population or is negative\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    483\u001b[0m result \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m k\n\u001b[0;32m    484\u001b[0m setsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m21\u001b[39m        \u001b[38;5;66;03m# size of a small set minus size of an empty list\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import random\n",
    "\n",
    "color_epochs = 1000\n",
    "color_batch_size = 30\n",
    "\n",
    "class ColorClassifer(nn.Module):\n",
    "    def __init__(self, latentSpaceDimensions):\n",
    "        super(ColorClassifer, self).__init__()\n",
    "        self.fc = nn.Linear(latentSpaceDimensions, 2).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "latentSpaceDimensions = len(next(iter(samples.values()))[0])\n",
    "classiferModel = ColorClassifer(latentSpaceDimensions).to(device)\n",
    "criterionClassiferModel = nn.CrossEntropyLoss().to(device)\n",
    "optimizerClassiferModel = optim.Adam(classiferModel.parameters(), lr=0.001)\n",
    "\n",
    "def randomSample(batch_size):\n",
    "    latent_vectors = random.sample(samples['red'], batch_size) + random.sample(samples['blue'], batch_size)\n",
    "    latent_vectors = torch.stack(latent_vectors).to(device)\n",
    "    labels = torch.tensor([0 for _ in range(batch_size)] + [1 for _ in range(batch_size)], dtype=torch.long).to(device)\n",
    "\n",
    "    assert(len(latent_vectors) == batch_size * 2)\n",
    "    assert(len(labels) == batch_size * 2)\n",
    "\n",
    "    return latent_vectors, labels\n",
    "\n",
    "for epoch in range(color_epochs):\n",
    "    latent_vectors, labels = randomSample(color_batch_size)\n",
    "    optimizerClassiferModel.zero_grad()\n",
    "    output = classiferModel(latent_vectors)\n",
    "    loss = criterionClassiferModel(output, labels)\n",
    "    if (epoch % 100 == 0):\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "    loss.backward()\n",
    "    optimizerClassiferModel.step()\n",
    "\n",
    "red_weight = classiferModel.state_dict()['fc.weight'][0].to(device)\n",
    "print(red_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight = -60\n",
    "sample_latent = samples['red'][0]\n",
    "altered_latent = sample_latent + red_weight * weight\n",
    "exportLatentToObj(altered_latent, 'color_test_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7edf6e3d3be48a0b7ccd3dffa7c28e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "blue_shoe = generateLatentFromPrompt('a blue shoe')\n",
    "altered_blue_shoe = blue_shoe + red_weight * weight\n",
    "exportLatentToObj(altered_blue_shoe, 'color_blue_shoe_test_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight = 60\n",
    "altered_blue_shoe = blue_shoe + red_weight * weight\n",
    "exportLatentToObj(altered_blue_shoe, 'color_blue_shoe_test_output')\n",
    "exportLatentToObj(blue_shoe, 'color_original_blue_shoe_test_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f3808e0d9a466faf9c5fb03d443ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b873f36e62cf405b87eb5b2323cb34a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "latentGuidance5 = generateLatentFromPrompt('a airplane', 5.0)\n",
    "latentGuidance50 = generateLatentFromPrompt('a airplane', 50.0)\n",
    "exportLatentToObj(latentGuidance5, 'latent_guidance_5')\n",
    "exportLatentToObj(latentGuidance50, 'latent_guidance_50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8beaf170d0fd4d6fbd9eab18fc8e6002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792fea316c724c208ec7cd3a5c1327d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "latentGuidance5 = generateLatentFromPrompt('a airplane', 1.0)\n",
    "latentGuidance50 = generateLatentFromPrompt('a airplane', 20.0)\n",
    "exportLatentToObj(latentGuidance5, 'latent_guidance_1')\n",
    "exportLatentToObj(latentGuidance50, 'latent_guidance_20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7e9685c3574a14a97ddf63976be5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "latent = generateLatentFromPrompt('a blue shoe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1048576])\n"
     ]
    }
   ],
   "source": [
    "print(latent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f2d4f41fbf409abf3112d39484b09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "latent = generateLatentFromPrompt('a blue backpack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\almog\\dev\\shap-e\\shap_e\\models\\stf\\renderer.py:286: UserWarning: exception rendering with PyTorch3D: No module named 'pytorch3d'\n",
      "  warnings.warn(f\"exception rendering with PyTorch3D: {exc}\")\n",
      "c:\\users\\almog\\dev\\shap-e\\shap_e\\models\\stf\\renderer.py:287: UserWarning: falling back on native PyTorch renderer, which does not support full gradients\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "exportLatentToObj(latent, 'a blue backpack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
